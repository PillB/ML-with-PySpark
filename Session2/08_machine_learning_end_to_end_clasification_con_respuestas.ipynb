{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"collapsed": true}, "outputs": [], "source": ["SANDBOX_NAME = # Sandbox Name\n", "DATA_PATH = \"/data/sandboxes/\" + SANDBOX_NAME + \"/data/\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "# Spark ML\n", "\n", "Cargamos un dataset con informaci\u00f3n de la distribuci\u00f3n de los p\u00edxeles para cada una de las letras del alfabeto escritas en may\u00fasculas. El objetivo ser\u00e1 predecir si el caracter en cuesti\u00f3n es una vocal o una consonante. En el proceso se aprender\u00e1n los pasos generales a seguir para solucionar un problema de este tipo con Spark ML.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Crear SparkSession"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.sql import SparkSession\n", "\n", "spark = SparkSession.builder.getOrCreate()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Cargar datos y comprobar schema"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Respuesta\n", "\n", "letters = spark.read.csv(DATA_PATH+'data/letter.txt', sep=',', header=True, inferSchema=True)"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["root\n", " |-- x_box_integer: integer (nullable = true)\n", " |-- y_box_integer: integer (nullable = true)\n", " |-- width_integer: integer (nullable = true)\n", " |-- high_integer: integer (nullable = true)\n", " |-- onpix_integer: integer (nullable = true)\n", " |-- x_bar_integer: integer (nullable = true)\n", " |-- y_bar_integer: integer (nullable = true)\n", " |-- x2bar_integer: integer (nullable = true)\n", " |-- y2bar_integer: integer (nullable = true)\n", " |-- xybar_integer: integer (nullable = true)\n", " |-- x2ybr_integer: integer (nullable = true)\n", " |-- xy2br_integer: integer (nullable = true)\n", " |-- x_ege_integer: integer (nullable = true)\n", " |-- xegvy_integer: integer (nullable = true)\n", " |-- y_ege_integer: integer (nullable = true)\n", " |-- yegvx_integer: integer (nullable = true)\n", " |-- class: string (nullable = true)\n", "\n"]}], "source": ["# Respuesta\n", "\n", "letters.printSchema()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Crear nueva variable objetivo\n", "\n", "La variable objetivo ahora mismo es cada una de las letras del abecedario en may\u00fascula. Se crea nueva variable con el nombre 'flag' que tome el valor 1 si se trata de una vocal y 0 en caso contrario, para convertir el problema en un problema de clasificaci\u00f3n binaria."]}, {"cell_type": "code", "execution_count": 5, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "import pyspark.sql.functions as F\n", "from pyspark.sql.types import DoubleType\n", "\n", "letters = letters.withColumn('tag', F.udf(lambda value: 1. if value in ['A', 'E', 'I', 'O', 'U'] else 0., DoubleType())(F.col('class')))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Primeros pasos\n", "\n", "Primeros pasos: nulos y vector assembler"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Empezaremos con la comprobaci\u00f3n y eliminaci\u00f3n nulos. Deber\u00edamos comprobar qu\u00e9 es lo que estamos eliminando, pero para la realizaci\u00f3n de este ejemplo, simplemente los eliminamos todos."]}, {"cell_type": "code", "execution_count": 6, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "for column in letters.columns:\n", "    letters = letters.where(F.col(column).isNotNull())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Tras remover todos los nulos, usamos el VectorAssembler con todas las variables excepto las objetivo (la nueva variable objetivo *tag* y la original *class*).\n", "\n", "Asumimos aqu\u00ed que todas las variables son num\u00e9ricas y que las queremos usar como input para nuestro modelo."]}, {"cell_type": "code", "execution_count": 7, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.feature import VectorAssembler\n", "\n", "vectorassembler = VectorAssembler(inputCols=[element for element in letters.columns if element != 'tag' and element !='class'], outputCol='assembled_features')\n", "letters = vectorassembler.transform(letters)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Selecci\u00f3n de variables"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Vamos a hacer lo que vimos en el Notebook de selecci\u00f3n de variables\n", "\n", "Vamos a tomar un n\u00famero de semillas aleatorias, y vamos a calcular la importancia de variables para cada una de las semillas. Una vez hecho esto, vamos a tomar como variables importantes aquellas que aparezcan en cada una de las iteraciones con las distintas semillas.\n", "\n", "Empezamos generando la lista de semillas aleatorias."]}, {"cell_type": "code", "execution_count": 8, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.classification import RandomForestClassifier\n", "\n", "random_seed = 4 \n", "num_iter = 10 #number of seeds we will use\n", "\n", "import random\n", "\n", "# Random seed for replicability, so every time we run it, it returns the same random values\n", "random.seed(random_seed)\n", "\n", "random_seeds=[] #list containing our random seeds\n", "\n", "# Get num_iter (10 in this example) random seeds and append them in our list random_seeds\n", "while len(set(random_seeds)) < num_iter:\n", "    random_seeds.append(random.randint(0,10000))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Creamos una lista *features_random_seed* que es una lista de listas, d\u00f3nde cada lista (1 por semilla, es decir, una por random forest) tiene tuplas con los nombres de variables y las importancias de las variables que explican el 95% de la importancia."]}, {"cell_type": "code", "execution_count": 9, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "features_random_seed = [] \n", "\n", "for random_seed in random_seeds:\n", "    rf = RandomForestClassifier(featuresCol=vectorassembler.getOutputCol(), labelCol='tag', seed = random_seed )\n", "    rf_model = rf.fit(letters)\n", "    \n", "    # We get the importances for the seed in this iteration\n", "    # We save the index to be able to get the variable name later\n", "    importances = [(index, value) for index, value in enumerate(rf_model.featureImportances.toArray().tolist())]\n", "\n", "    # Sort from higher to lower relevance\n", "    importances = sorted(importances, key=lambda value: value[1], reverse=True)\n", "    \n", "    # We keep the ones that explain the 95% of importance regarding the target variable\n", "\n", "    compt = 0\n", "    important_features =[]\n", "    for element in importances:\n", "        if compt < 0.95:\n", "            compt += element[1]\n", "            important_features.append((vectorassembler.getInputCols()[element[0]], element[1]))\n", "    features_random_seed.append(important_features)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Creamos una lista *features_all_seeds* que es una lista de variables que aparecieron como importantes en cada iteraci\u00f3n de las semillas"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "flat_features = [feature for one_seed in features_random_seed for feature in one_seed]\n", "features = [element[0] for element in flat_features]\n", "# We transform our list of lists to just one list \n", "\n", "# We use Counter to get the variables which appear as important in every iteration\n", "# Elements are stored as dictionary keys and their counts are stored as dictionary values.\n", "from collections import Counter\n", "\n", "features_all_seeds = [element[0] for element in Counter(features).items() if element[1] == num_iter]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Ahora creamos *dicitonary_importances* que tendr\u00e1 el nombre de cada variable (que apareci\u00f3 como importante en todos los RF) y la media de la importancia que obtuvo para todas las semillas"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"collapsed": false}, "outputs": [{"data": {"text/plain": ["[('xy2br_integer', 0.17033443008368546),\n", " ('y_bar_integer', 0.14201330587864597),\n", " ('width_integer', 0.10679107744754845),\n", " ('x_ege_integer', 0.10426602229681688),\n", " ('x2ybr_integer', 0.08891062630343921),\n", " ('x2bar_integer', 0.08847049838692729),\n", " ('y2bar_integer', 0.08702226475991735),\n", " ('xegvy_integer', 0.06031182687195572),\n", " ('y_ege_integer', 0.04687976572650596)]"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["# Respuesta\n", "\n", "import numpy as np\n", "\n", "dictionary_importances = {}\n", "\n", "for feature in features_all_seeds: # has the important variables which appeared in all RF\n", "    dictionary_importances[feature] = []\n", "    \n", "    # Features_random_seed is a list of lists (1/RF), where each list has tuples with variables and importances\n", "    for values in features_random_seed:\n", "        for element in values: # values is each list of a RF with tuples\n", "            if element[0] == feature: # element is each tuple (variable, importance)\n", "                dictionary_importances[feature].append(element[1]) # append importance values of each RF\n", "                break\n", "    dictionary_importances[feature] = np.mean(dictionary_importances[feature]) # get the mean of the importance\n", "\n", "dictionary_importances = sorted(dictionary_importances.items(), key=lambda value: value[1], reverse=True)\n", "dictionary_importances"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Mostrar las variables m\u00e1s importantes"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"collapsed": false}, "outputs": [{"data": {"text/plain": ["['xegvy_integer',\n", " 'width_integer',\n", " 'y2bar_integer',\n", " 'x2bar_integer',\n", " 'y_bar_integer',\n", " 'xy2br_integer',\n", " 'y_ege_integer',\n", " 'x2ybr_integer',\n", " 'x_ege_integer']"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["# Respuesta\n", "\n", "features_all_seeds"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Hemos visto las variables m\u00e1s importantes, podr\u00edamos intentar usar s\u00f3lo esas para nuestro modelo viendo cu\u00e1l es el resultado de realizar eso. En esta caso, vamos a continuar con todas.\n", "\n", "Estandarizamos los valores y lanzamos un modelo de clasificaci\u00f3n dentro de un Pipeline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "**Regresi\u00f3n Log\u00edstica** \n", "\n", "Vamos a hacer primero el vector assembler para alimentar este al StandardScaler, y la salida del mismo ser\u00e1 el input para nuestro modelo.\n", "\n", "Esto lo hacemos utilizando un Pipeline como ya vimos anteriormente (haciendo el fit y transform s\u00f3lo en el pipeline)."]}, {"cell_type": "code", "execution_count": 13, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+-------------+-------------+-------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+---+--------------------+----------------------------+---------------------+--------------------+--------------------+----------+\n", "|x_box_integer|y_box_integer|width_integer|high_integer|onpix_integer|x_bar_integer|y_bar_integer|x2bar_integer|y2bar_integer|xybar_integer|x2ybr_integer|xy2br_integer|x_ege_integer|xegvy_integer|y_ege_integer|yegvx_integer|class|tag|  assembled_features|assembled_important_features|standardized_features|       rawPrediction|         probability|prediction|\n", "+-------------+-------------+-------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+---+--------------------+----------------------------+---------------------+--------------------+--------------------+----------+\n", "|            0|            0|            0|           0|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|(16,[5,6,7,8,9,10...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "|            0|            0|            0|           0|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|(16,[5,6,7,8,9,10...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "|            0|            0|            0|           0|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|(16,[5,6,7,8,9,10...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "|            0|            0|            0|           1|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,0.0,0.0,1.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "|            0|            0|            0|           1|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,0.0,0.0,1.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "|            0|            0|            1|           0|            0|            3|           12|            4|            2|           11|            9|            6|            0|            8|            2|            8|    F|0.0|[0.0,0.0,1.0,0.0,...|        [8.0,1.0,2.0,4.0,...| [5.17849615490882...|[2.23806228916567...|[0.90361582672749...|       0.0|\n", "|            0|            0|            1|           0|            0|            5|           10|            6|            1|            9|            6|            5|            1|            9|            2|            8|    P|0.0|[0.0,0.0,1.0,0.0,...|        [9.0,1.0,1.0,6.0,...| [5.82580817427242...|[2.22425462087797...|[0.90240653786848...|       0.0|\n", "|            0|            0|            1|           0|            0|            6|            7|            5|            6|            7|            6|           13|            0|            8|            3|           10|    C|0.0|[0.0,0.0,1.0,0.0,...|        [8.0,1.0,6.0,5.0,...| [5.17849615490882...|[0.19592672391039...|[0.54882559067524...|       0.0|\n", "|            0|            0|            1|           0|            0|            7|           13|            1|            4|            7|           10|            8|            0|            8|            0|            8|    T|0.0|[0.0,0.0,1.0,0.0,...|        [8.0,1.0,4.0,1.0,...| [5.17849615490882...|[1.97334557594822...|[0.87797000724316...|       0.0|\n", "|            0|            0|            1|           0|            0|           12|            4|            5|            3|           12|            5|           11|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,0.0,...|        [7.0,1.0,3.0,5.0,...| [4.53118413554522...|[-0.5682681329670...|[0.36163654009019...|       1.0|\n", "|            0|            0|            1|           1|            0|            5|            7|            5|            7|            7|            6|           12|            0|            8|            6|           10|    E|1.0|[0.0,0.0,1.0,1.0,...|        [8.0,1.0,7.0,5.0,...| [5.17849615490882...|[0.46549845702766...|[0.61431774965185...|       0.0|\n", "|            0|            0|            1|           1|            0|           12|            4|            4|            3|           12|            4|           10|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,1.0,...|        [7.0,1.0,3.0,4.0,...| [4.53118413554522...|[-0.3883681118544...|[0.40411020619911...|       1.0|\n", "|            0|            0|            1|           1|            0|           12|            4|            5|            3|           12|            4|           10|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,1.0,...|        [7.0,1.0,3.0,5.0,...| [4.53118413554522...|[-0.3502465846771...|[0.41332262621824...|       1.0|\n", "|            0|            0|            1|           1|            0|           12|            4|            5|            3|           12|            4|           11|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,1.0,...|        [7.0,1.0,3.0,5.0,...| [4.53118413554522...|[-0.4737755136188...|[0.38372302034805...|       1.0|\n", "|            0|            0|            1|           1|            0|           12|            4|            6|            3|           12|            5|           11|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,1.0,...|        [7.0,1.0,3.0,6.0,...| [4.53118413554522...|[-0.5301466057897...|[0.37048269520885...|       1.0|\n", "|            0|            1|            0|           1|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,1.0,0.0,1.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "|            0|            1|            1|           2|            0|            7|            7|            2|            6|            7|            6|            8|            0|            8|            2|            8|    I|1.0|[0.0,1.0,1.0,2.0,...|        [8.0,1.0,6.0,2.0,...| [5.17849615490882...|[0.64846874829089...|[0.65666531568668...|       0.0|\n", "|            0|            3|            0|           4|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,3.0,0.0,4.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "|            0|            4|            0|           6|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,4.0,0.0,6.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "|            0|            7|            0|           5|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,7.0,0.0,5.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[0.51206818502388...|[0.62529118003122...|       0.0|\n", "+-------------+-------------+-------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+---+--------------------+----------------------------+---------------------+--------------------+--------------------+----------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["# Respuesta\n", "\n", "from pyspark.ml import Pipeline\n", "from pyspark.ml.feature import StandardScaler\n", "from pyspark.ml.classification import LogisticRegression\n", "\n", "vector_assembler = VectorAssembler(inputCols=features_all_seeds, outputCol='assembled_important_features')\n", "standard_scaler = StandardScaler(inputCol=vector_assembler.getOutputCol(), outputCol='standardized_features')\n", "log_reg = LogisticRegression(featuresCol=standard_scaler.getOutputCol(), labelCol='tag')\n", "\n", "pipeline_log_reg = Pipeline(stages=[vector_assembler, standard_scaler, log_reg])\n", "\n", "letters_train, letters_test = letters.randomSplit([0.8,0.2], seed=4)\n", "\n", "pipeline_model_log_reg = pipeline_log_reg.fit(letters_train)\n", "\n", "letters_test_log_reg = pipeline_model_log_reg.transform(letters_test)\n", "\n", "letters_test_log_reg.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "**RandomForest**\n", "\n", "Hacemos lo mismo que con la regresi\u00f3n log\u00edstica pero utilizando como modelo el Random Forest."]}, {"cell_type": "code", "execution_count": 14, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+-------------+-------------+-------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+---+--------------------+----------------------------+---------------------+--------------------+--------------------+----------+\n", "|x_box_integer|y_box_integer|width_integer|high_integer|onpix_integer|x_bar_integer|y_bar_integer|x2bar_integer|y2bar_integer|xybar_integer|x2ybr_integer|xy2br_integer|x_ege_integer|xegvy_integer|y_ege_integer|yegvx_integer|class|tag|  assembled_features|assembled_important_features|standardized_features|       rawPrediction|         probability|prediction|\n", "+-------------+-------------+-------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+---+--------------------+----------------------------+---------------------+--------------------+--------------------+----------+\n", "|            0|            0|            0|           0|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|(16,[5,6,7,8,9,10...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "|            0|            0|            0|           0|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|(16,[5,6,7,8,9,10...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "|            0|            0|            0|           0|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|(16,[5,6,7,8,9,10...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "|            0|            0|            0|           1|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,0.0,0.0,1.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "|            0|            0|            0|           1|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,0.0,0.0,1.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "|            0|            0|            1|           0|            0|            3|           12|            4|            2|           11|            9|            6|            0|            8|            2|            8|    F|0.0|[0.0,0.0,1.0,0.0,...|        [8.0,1.0,2.0,4.0,...| [5.17849615490882...|[19.0112940961580...|[0.95056470480790...|       0.0|\n", "|            0|            0|            1|           0|            0|            5|           10|            6|            1|            9|            6|            5|            1|            9|            2|            8|    P|0.0|[0.0,0.0,1.0,0.0,...|        [9.0,1.0,1.0,6.0,...| [5.82580817427242...|[18.8192259328090...|[0.94096129664045...|       0.0|\n", "|            0|            0|            1|           0|            0|            6|            7|            5|            6|            7|            6|           13|            0|            8|            3|           10|    C|0.0|[0.0,0.0,1.0,0.0,...|        [8.0,1.0,6.0,5.0,...| [5.17849615490882...|[13.2365581106074...|[0.66182790553037...|       0.0|\n", "|            0|            0|            1|           0|            0|            7|           13|            1|            4|            7|           10|            8|            0|            8|            0|            8|    T|0.0|[0.0,0.0,1.0,0.0,...|        [8.0,1.0,4.0,1.0,...| [5.17849615490882...|[17.8168881270453...|[0.89084440635226...|       0.0|\n", "|            0|            0|            1|           0|            0|           12|            4|            5|            3|           12|            5|           11|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,0.0,...|        [7.0,1.0,3.0,5.0,...| [4.53118413554522...|[14.5652610861320...|[0.72826305430660...|       0.0|\n", "|            0|            0|            1|           1|            0|            5|            7|            5|            7|            7|            6|           12|            0|            8|            6|           10|    E|1.0|[0.0,0.0,1.0,1.0,...|        [8.0,1.0,7.0,5.0,...| [5.17849615490882...|[12.2446250011063...|[0.61223125005531...|       0.0|\n", "|            0|            0|            1|           1|            0|           12|            4|            4|            3|           12|            4|           10|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,1.0,...|        [7.0,1.0,3.0,4.0,...| [4.53118413554522...|[14.6414808054608...|[0.73207404027304...|       0.0|\n", "|            0|            0|            1|           1|            0|           12|            4|            5|            3|           12|            4|           10|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,1.0,...|        [7.0,1.0,3.0,5.0,...| [4.53118413554522...|[14.5652610861320...|[0.72826305430660...|       0.0|\n", "|            0|            0|            1|           1|            0|           12|            4|            5|            3|           12|            4|           11|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,1.0,...|        [7.0,1.0,3.0,5.0,...| [4.53118413554522...|[14.5652610861320...|[0.72826305430660...|       0.0|\n", "|            0|            0|            1|           1|            0|           12|            4|            6|            3|           12|            5|           11|            0|            7|            0|            8|    J|0.0|[0.0,0.0,1.0,1.0,...|        [7.0,1.0,3.0,6.0,...| [4.53118413554522...|[14.5652610861320...|[0.72826305430660...|       0.0|\n", "|            0|            1|            0|           1|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,1.0,0.0,1.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "|            0|            1|            1|           2|            0|            7|            7|            2|            6|            7|            6|            8|            0|            8|            2|            8|    I|1.0|[0.0,1.0,1.0,2.0,...|        [8.0,1.0,6.0,2.0,...| [5.17849615490882...|[13.1876720566770...|[0.65938360283385...|       0.0|\n", "|            0|            3|            0|           4|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,3.0,0.0,4.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "|            0|            4|            0|           6|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,4.0,0.0,6.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "|            0|            7|            0|           5|            0|            7|            7|            4|            4|            7|            6|            8|            0|            8|            0|            8|    I|1.0|[0.0,7.0,0.0,5.0,...|        [8.0,0.0,4.0,4.0,...| [5.17849615490882...|[1.73978899180761...|[0.08698944959038...|       1.0|\n", "+-------------+-------------+-------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+---+--------------------+----------------------------+---------------------+--------------------+--------------------+----------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["# Respuesta\n", "\n", "from pyspark.ml.classification import RandomForestClassifier\n", "\n", "vector_assembler = VectorAssembler(inputCols=features_all_seeds, outputCol='assembled_important_features')\n", "standard_scaler = StandardScaler(inputCol=vector_assembler.getOutputCol(), outputCol='standardized_features')\n", "rf = RandomForestClassifier(featuresCol=standard_scaler.getOutputCol(), labelCol='tag')\n", "\n", "pipeline = Pipeline(stages=[vector_assembler, standard_scaler, rf])\n", "\n", "pipeline_model_rf = pipeline.fit(letters_train)\n", "\n", "letters_test_rf = pipeline_model_rf.transform(letters_test)\n", "\n", "letters_test_rf.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Evaluar modelos para decidir cu\u00e1l predice mejor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Regresi\u00f3n Log\u00edstica"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Importar librer\u00edas necesarias. Podemos observar que usamos tambi\u00e9n mllib (recordemos que con dicha librer\u00eda debemos trabajar con rdd en lugar de dataframe) para obtener el objeto MulticlassMetrics que usaremos para calcular el recall, precision, f1 score y la matriz de confusi\u00f3n."]}, {"cell_type": "code", "execution_count": 15, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n", "from pyspark.mllib.evaluation import MulticlassMetrics"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "AUC"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"collapsed": false}, "outputs": [{"data": {"text/plain": ["0.7269502483778738"]}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": ["# Respuesta\n", "\n", "auc = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='tag', metricName='areaUnderROC')\n", "auc.evaluate(letters_test_log_reg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Otras m\u00e9tricas"]}, {"cell_type": "code", "execution_count": 17, "metadata": {"collapsed": false}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/opt/spark/dist/python/pyspark/mllib/evaluation.py:262: UserWarning: Deprecated in 2.0.0. Use accuracy.\n", "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Recall: 0.061124694376528114\n", "Precision: 0.3816793893129771\n", "f1: 0.7878030492376906\n", "Confusion matrix: DenseMatrix([[3102.,   81.],\n", "             [ 768.,   50.]])\n"]}], "source": ["# Respuesta\n", "\n", "metrics = MulticlassMetrics(letters_test_log_reg.select('prediction', 'tag').rdd)\n", "\n", "recall = metrics.recall(label=1)\n", "precision = metrics.precision(label=1)\n", "f1 = metrics.fMeasure()\n", "confusion_matrix = metrics.confusionMatrix()\n", "\n", "print(\"Recall: {}\".format(recall))\n", "print(\"Precision: {}\".format(precision))\n", "print(\"f1: {}\".format(f1))\n", "print(\"Confusion matrix: {}\".format(confusion_matrix))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Random Forest"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Realizamos lo mismo que con la regresi\u00f3n log\u00edstica, calculando: AUC, otras m\u00e9tricas (recall, precision, f1 score) y matriz de confusi\u00f3n"]}, {"cell_type": "code", "execution_count": 18, "metadata": {"collapsed": false}, "outputs": [{"data": {"text/plain": ["0.8787438155174935"]}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": ["# Respuesta\n", "\n", "auc = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='tag', metricName='areaUnderROC')\n", "auc.evaluate(letters_test_rf)"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"collapsed": false}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/opt/spark/dist/python/pyspark/mllib/evaluation.py:262: UserWarning: Deprecated in 2.0.0. Use accuracy.\n", "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Recall: 0.26894865525672373\n", "Precision: 0.9777777777777777\n", "f1: 0.8492876780804799\n", "Confusion matrix: DenseMatrix([[3178.,    5.],\n", "             [ 598.,  220.]])\n"]}], "source": ["# Respuesta\n", "\n", "metrics = MulticlassMetrics(letters_test_rf.select('prediction', 'tag').rdd)\n", "\n", "recall = metrics.recall(label=1)\n", "precision = metrics.precision(label=1)\n", "f1 = metrics.fMeasure()\n", "confusion_matrix = metrics.confusionMatrix()\n", "\n", "print(\"Recall: {}\".format(recall))\n", "print(\"Precision: {}\".format(precision))\n", "print(\"f1: {}\".format(f1))\n", "print(\"Confusion matrix: {}\".format(confusion_matrix))"]}], "metadata": {"@webio": {"lastCommId": "0f1cc61b6bb340ecac2a5cea7f61b70a", "lastKernelId": "6f462f9a-2121-4a15-9d4f-d6566384cce2"}, "kernelspec": {"display_name": "PySpark Python3 - Spark 2.1.0", "language": "python", "name": "spark2python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.2"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}