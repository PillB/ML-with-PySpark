{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SANDBOX_NAME = # Sandbox Name\n",
    "DATA_PATH = \"/data/sandboxes/\"+SANDBOX_NAME+\"/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Ejemplo de Param Grid\n",
    "\n",
    "Para encontrar los mejores hiperaparámetros para un modelo, se puede definir un conjunto de posibles valores para cada hiperparámetro, y crear un programa que entrene modelos con cada combinación posible de ellos, y almacene el mejor modelo dada una metrica. Además se puede mejorar junto con la técnica de Validación Cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Primer paso, cargar algunos datos de prueba e inspeccionar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "bank = spark.read.csv(DATA_PATH+'bank.csv', sep=';', header=True, inferSchema=True)\n",
    "\n",
    "bank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n",
      "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n",
      "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n",
      "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n",
      "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "bank.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Busquemos valores nulos en todas las columnas y descartemos filas que tengan nulos en ellas. Ya vimos anteriormente cómo trabajar con valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for nulls at age\n",
      "-> None null found.\n",
      "Looking for nulls at job\n",
      "-> None null found.\n",
      "Looking for nulls at marital\n",
      "-> None null found.\n",
      "Looking for nulls at education\n",
      "-> None null found.\n",
      "Looking for nulls at default\n",
      "-> None null found.\n",
      "Looking for nulls at balance\n",
      "-> None null found.\n",
      "Looking for nulls at housing\n",
      "-> None null found.\n",
      "Looking for nulls at loan\n",
      "-> None null found.\n",
      "Looking for nulls at contact\n",
      "-> None null found.\n",
      "Looking for nulls at day\n",
      "-> None null found.\n",
      "Looking for nulls at month\n",
      "-> None null found.\n",
      "Looking for nulls at duration\n",
      "-> None null found.\n",
      "Looking for nulls at campaign\n",
      "-> None null found.\n",
      "Looking for nulls at pdays\n",
      "-> None null found.\n",
      "Looking for nulls at previous\n",
      "-> None null found.\n",
      "Looking for nulls at poutcome\n",
      "-> None null found.\n",
      "Looking for nulls at y\n",
      "-> None null found.\n"
     ]
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "for column in bank.columns:\n",
    "    print(\"Looking for nulls at \" +column)\n",
    "    num_nulls = bank.where(F.col(column).isNull()).count()\n",
    "    if  num_nulls != 0:\n",
    "        print(\"There are null values in the column {}\".format(column))\n",
    "        bank = bank.where(F.col(column).isNotNull())\n",
    "        if num_nulls == 0:\n",
    "            print(\"The column {} is free from null values\".format(column))\n",
    "    else:\n",
    "        print(\"-> None null found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Tras limpiar el dataset de nulos, podemos continuar preparando las variables y entrenando un modelo.\n",
    "Para mantenerlo sencillo, apuntaremos las columnas binarias para evitar aplicarles onehot.\n",
    "\n",
    "- Aplicamos el string indexer a todas las columnas tipo string (estamos asumiendo aquí que todas las columnas tipo string son categóricas)\n",
    "- Aplicamos one hot encoder a todas las variables string no binarias\n",
    "- Tras el string indexer Y el one hot encoder en las variables no binarias, removemos la columna resultado del string indexer para quedarnos sólo con la salida del one hot encoder. Y le cambiamos el nombre a la salida del one hot encoder a *_encoded*. Así sólo tendremos una variable *_encoded* para cada variable transformada en lugar de tener en el dataset el resultado del string indexer Y el del one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job\n",
      "marital\n",
      "education\n",
      "default\n",
      "housing\n",
      "loan\n",
      "contact\n",
      "month\n",
      "poutcome\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "binary_columns = [\"default\",\"housing\",\"loan\",\"y\"]\n",
    "\n",
    "string_columns = [item[0] for item in bank.dtypes if item[1].startswith('string')]\n",
    "\n",
    "# making a copy of our dataset\n",
    "bank_many_steps = bank.select(\"*\")\n",
    "\n",
    "for col in string_columns:\n",
    "    print(col)\n",
    "    string_indexer = StringIndexer(inputCol=col, outputCol=col+\"_encoded\")\n",
    "    string_indexer_model = string_indexer.fit(bank_many_steps)\n",
    "    bank_many_steps = string_indexer_model.transform(bank_many_steps)\n",
    "    \n",
    "    if col not in binary_columns:\n",
    "        onehotencoder = OneHotEncoder(dropLast=False, inputCol= string_indexer.getOutputCol(), outputCol=col+\"_encoded_tmp\")\n",
    "        \n",
    "        bank_many_steps = onehotencoder.transform(bank_many_steps)\n",
    "        bank_many_steps = bank_many_steps.drop(string_indexer.getOutputCol())\n",
    "        bank_many_steps = bank_many_steps.withColumnRenamed(onehotencoder.getOutputCol(),string_indexer.getOutputCol())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+--------------+---------------+-----------------+---------------+---------------+------------+---------------+--------------+----------------+---------+--------------------+\n",
      "|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|   job_encoded|marital_encoded|education_encoded|default_encoded|housing_encoded|loan_encoded|contact_encoded| month_encoded|poutcome_encoded|y_encoded|  assembled_features|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+--------------+---------------+-----------------+---------------+---------------+------------+---------------+--------------+----------------+---------+--------------------+\n",
      "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|(12,[8],[1.0])|  (3,[0],[1.0])|    (4,[2],[1.0])|            0.0|            1.0|         0.0|  (3,[0],[1.0])|(12,[8],[1.0])|   (4,[0],[1.0])|      0.0|(48,[0,1,2,3,4,5,...|\n",
      "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|(12,[4],[1.0])|  (3,[0],[1.0])|    (4,[0],[1.0])|            0.0|            0.0|         1.0|  (3,[0],[1.0])|(12,[0],[1.0])|   (4,[1],[1.0])|      0.0|(48,[0,1,2,3,4,5,...|\n",
      "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|(12,[0],[1.0])|  (3,[1],[1.0])|    (4,[1],[1.0])|            0.0|            0.0|         0.0|  (3,[0],[1.0])|(12,[5],[1.0])|   (4,[1],[1.0])|      0.0|(48,[0,1,2,3,4,5,...|\n",
      "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|(12,[0],[1.0])|  (3,[0],[1.0])|    (4,[1],[1.0])|            0.0|            0.0|         1.0|  (3,[1],[1.0])|(12,[3],[1.0])|   (4,[0],[1.0])|      0.0|(48,[0,1,2,3,4,5,...|\n",
      "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|(12,[1],[1.0])|  (3,[0],[1.0])|    (4,[0],[1.0])|            0.0|            0.0|         0.0|  (3,[1],[1.0])|(12,[0],[1.0])|   (4,[0],[1.0])|      0.0|(48,[0,2,3,4,5,8,...|\n",
      "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+--------------+---------------+-----------------+---------------+---------------+------------+---------------+--------------+----------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "bank_many_steps.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Listamos columnas de entrenamiento y seleccionamos columna de target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "target_column = \"y_encoded\"\n",
    "numeric_columns = [element[0] for element in bank_many_steps.dtypes if element[1] != 'string' not in element[0]]\n",
    "columns_for_model = [c for c in numeric_columns if c!=target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'balance',\n",
       " 'day',\n",
       " 'duration',\n",
       " 'campaign',\n",
       " 'pdays',\n",
       " 'previous',\n",
       " 'job_encoded',\n",
       " 'marital_encoded',\n",
       " 'education_encoded',\n",
       " 'default_encoded',\n",
       " 'housing_encoded',\n",
       " 'loan_encoded',\n",
       " 'contact_encoded',\n",
       " 'month_encoded',\n",
       " 'poutcome_encoded']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "columns_for_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Primera aproximación para crear un modelo: crear obligatorio VectorAssembler, dividir los datos en train y test y entrenar la primera versión con hipermarámetros por defecto.\n",
    "\n",
    "Y evaluamos nuestro modelo (usaremos accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=columns_for_model, outputCol='assembled_features')\n",
    "bank_many_steps = vector_assembler.transform(bank_many_steps)\n",
    "\n",
    "bank_many_steps_train, bank_many_steps_test = bank_many_steps.randomSplit([0.8,0.2])\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=vector_assembler.getOutputCol(), labelCol=target_column)\n",
    "rf_model = rf.fit(bank_many_steps_train)\n",
    "bank_many_steps_prediction = rf_model.transform(bank_many_steps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=rf.getPredictionCol(),\n",
    "                                              labelCol=rf.getLabelCol(),\n",
    "                                             metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8928176795580111"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "evaluator.evaluate(bank_many_steps_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Podemos ver el número de árboles de nuestro random forest usando el atributo getNumTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "rf_model.getNumTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Pero, ¿podemos afirmar que este modelo fue entrenado con los mejores hiperparámetros? Se podrían listar muchas combinaciones de ellos, lo que significa mucho trabajo manual. Pero no hay que preocuparse, para manejar esta situación Spark incorpora **ParamGrid**. \n",
    "\n",
    "Sólo hay que definir los valores de cada hiperparámetro que queremos probar en ParamGridBuilder y utilizar el objeto resultante en el parámetro `estimatorParamMaps` cuando se define el CrossValidator.\n",
    "\n",
    "En este caso, probaremos todas las combinaciones de:\n",
    "\n",
    "- Number of trees: [10,50,100,200]\n",
    "- Max depth: [3, 5, 7]\n",
    "- Min instances per node: [3, 5, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(rf.numTrees, [10,50,100,200]) \\\n",
    "                                .addGrid(rf.maxDepth, [3,5,7]) \\\n",
    "                                .addGrid(rf.minInstancesPerNode, [3,5,7]) \\\n",
    "                                .build()\n",
    "rf_cv = CrossValidator(estimator=rf, \n",
    "                       estimatorParamMaps=grid, \n",
    "                       evaluator=evaluator, \n",
    "                       numFolds=3)\n",
    "rf_cv_model = rf_cv.fit(bank_many_steps_test)\n",
    "\n",
    "# In the attribute bestModel we have the best model after trying all the possible combinations of \n",
    "# hyperparameter values in a random forest, using accuracy as our metric and doing cross validation with 3 folds\n",
    "bestModel = rf_cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "bestModel.getNumTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8928176795580111"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "evaluator.evaluate(bank_many_steps_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Podemos encontrar todos los parámetros en el JavaObject del objeto modelo de pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Respuesta\n",
    "\n",
    "bestModel._java_obj.getMinInstancesPerNode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
