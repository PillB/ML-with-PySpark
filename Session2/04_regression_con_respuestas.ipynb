{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"collapsed": true}, "outputs": [], "source": ["SANDBOX_NAME = # Sandbox Name\n", "DATA_PATH = \"/data/sandboxes/\"+SANDBOX_NAME+\"/data/data/\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "# Spark ML Problema de Regresi\u00f3n\n", "\n", "En este notebook abordaremos el problema de Machine Learning Supervisado de regresi\u00f3n. Trabajaremos con el dataset Boston Housing que contiene informaci\u00f3n sobre las diferentes caracter\u00edsticas de casas en la ciudad de Boston. Utilizaremos como variable objetivo el precio de las casas. Accederemos a este a trav\u00e9s de la librer\u00eda de ML de Python scikit-learn. Ajustaremos distintos modelos y compararemos los resultados obtenidos en \u00e9stos."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Crear SparkSession\n", "Nota: en Datio no es necesario crear la sesi\u00f3n de Spark ya al iniciar un notebook con el kernel PySpark Python3 - Spark 2.1.0 se crea autom\u00e1ticamente."]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Respuesta\n", "from pyspark.sql import SparkSession\n", "spark = SparkSession.builder.getOrCreate()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Cargamos los datos en un DataFrame de Spark\n", "Cargamos los datos de scikit-learn y los consolidamos en un DataFrame de Spark."]}, {"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n", "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|MEDV|\n", "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n", "|0.00632|18.0| 2.31| 0.0|0.538|6.575| 65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98|24.0|\n", "|0.02731| 0.0| 7.07| 0.0|0.469|6.421| 78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14|21.6|\n", "|0.02729| 0.0| 7.07| 0.0|0.469|7.185| 61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03|34.7|\n", "|0.03237| 0.0| 2.18| 0.0|0.458|6.998| 45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94|33.4|\n", "|0.06905| 0.0| 2.18| 0.0|0.458|7.147| 54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33|36.2|\n", "|0.02985| 0.0| 2.18| 0.0|0.458| 6.43| 58.7|6.0622|3.0|222.0|   18.7|394.12| 5.21|28.7|\n", "|0.08829|12.5| 7.87| 0.0|0.524|6.012| 66.6|5.5605|5.0|311.0|   15.2| 395.6|12.43|22.9|\n", "|0.14455|12.5| 7.87| 0.0|0.524|6.172| 96.1|5.9505|5.0|311.0|   15.2| 396.9|19.15|27.1|\n", "|0.21124|12.5| 7.87| 0.0|0.524|5.631|100.0|6.0821|5.0|311.0|   15.2|386.63|29.93|16.5|\n", "|0.17004|12.5| 7.87| 0.0|0.524|6.004| 85.9|6.5921|5.0|311.0|   15.2|386.71| 17.1|18.9|\n", "|0.22489|12.5| 7.87| 0.0|0.524|6.377| 94.3|6.3467|5.0|311.0|   15.2|392.52|20.45|15.0|\n", "|0.11747|12.5| 7.87| 0.0|0.524|6.009| 82.9|6.2267|5.0|311.0|   15.2| 396.9|13.27|18.9|\n", "|0.09378|12.5| 7.87| 0.0|0.524|5.889| 39.0|5.4509|5.0|311.0|   15.2| 390.5|15.71|21.7|\n", "|0.62976| 0.0| 8.14| 0.0|0.538|5.949| 61.8|4.7075|4.0|307.0|   21.0| 396.9| 8.26|20.4|\n", "|0.63796| 0.0| 8.14| 0.0|0.538|6.096| 84.5|4.4619|4.0|307.0|   21.0|380.02|10.26|18.2|\n", "|0.62739| 0.0| 8.14| 0.0|0.538|5.834| 56.5|4.4986|4.0|307.0|   21.0|395.62| 8.47|19.9|\n", "|1.05393| 0.0| 8.14| 0.0|0.538|5.935| 29.3|4.4986|4.0|307.0|   21.0|386.85| 6.58|23.1|\n", "| 0.7842| 0.0| 8.14| 0.0|0.538| 5.99| 81.7|4.2579|4.0|307.0|   21.0|386.75|14.67|17.5|\n", "|0.80271| 0.0| 8.14| 0.0|0.538|5.456| 36.6|3.7965|4.0|307.0|   21.0|288.99|11.69|20.2|\n", "| 0.7258| 0.0| 8.14| 0.0|0.538|5.727| 69.5|3.7965|4.0|307.0|   21.0|390.95|11.28|18.2|\n", "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n", "only showing top 20 rows\n", "\n"]}], "source": ["# Respuesta\n", "\n", "import pandas as pd\n", "from sklearn.datasets import load_boston\n", "\n", "boston_dataset = load_boston()\n", "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n", "# We add the target variable to our Pandas DataFrame\n", "boston['MEDV'] = boston_dataset.target\n", "# We create a Spark DataFrame from the Pandas DataFrame\n", "boston = spark.createDataFrame(boston)\n", "boston.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Vemos el schema."]}, {"cell_type": "code", "execution_count": 4, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["root\n", " |-- CRIM: double (nullable = true)\n", " |-- ZN: double (nullable = true)\n", " |-- INDUS: double (nullable = true)\n", " |-- CHAS: double (nullable = true)\n", " |-- NOX: double (nullable = true)\n", " |-- RM: double (nullable = true)\n", " |-- AGE: double (nullable = true)\n", " |-- DIS: double (nullable = true)\n", " |-- RAD: double (nullable = true)\n", " |-- TAX: double (nullable = true)\n", " |-- PTRATIO: double (nullable = true)\n", " |-- B: double (nullable = true)\n", " |-- LSTAT: double (nullable = true)\n", " |-- MEDV: double (nullable = true)\n", "\n"]}], "source": ["# Respuesta\n", "boston.printSchema()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Podemos ver la descripci\u00f3n de las variables del objeto cargado de scikit-learn."]}, {"cell_type": "code", "execution_count": 5, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": [".. _boston_dataset:\n", "\n", "Boston house prices dataset\n", "---------------------------\n", "\n", "**Data Set Characteristics:**  \n", "\n", "    :Number of Instances: 506 \n", "\n", "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n", "\n", "    :Attribute Information (in order):\n", "        - CRIM     per capita crime rate by town\n", "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n", "        - INDUS    proportion of non-retail business acres per town\n", "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n", "        - NOX      nitric oxides concentration (parts per 10 million)\n", "        - RM       average number of rooms per dwelling\n", "        - AGE      proportion of owner-occupied units built prior to 1940\n", "        - DIS      weighted distances to five Boston employment centres\n", "        - RAD      index of accessibility to radial highways\n", "        - TAX      full-value property-tax rate per $10,000\n", "        - PTRATIO  pupil-teacher ratio by town\n", "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n", "        - LSTAT    % lower status of the population\n", "        - MEDV     Median value of owner-occupied homes in $1000's\n", "\n", "    :Missing Attribute Values: None\n", "\n", "    :Creator: Harrison, D. and Rubinfeld, D.L.\n", "\n", "This is a copy of UCI ML housing dataset.\n", "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n", "\n", "\n", "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n", "\n", "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n", "prices and the demand for clean air', J. Environ. Economics & Management,\n", "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n", "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n", "pages 244-261 of the latter.\n", "\n", "The Boston house-price data has been used in many machine learning papers that address regression\n", "problems.   \n", "     \n", ".. topic:: References\n", "\n", "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n", "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n", "\n"]}], "source": ["# Respuesta\n", "\n", "print(boston_dataset.DESCR)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Pasos previos\n", "\n", "#### Vector Assembler\n", "Para ajustar un modelo en Spark necesitamos indicar qu\u00e9 variables se van a utilizar como variables independientes. A trav\u00e9s del par\u00e1metro featuresCol de los distintos algoritmos, se le indica la variable que contiene la salida del  VectorAssembler con las variables independientes. \n", "\n", "Hacemos el VectorAssembler con todas las variables con excepcion del target."]}, {"cell_type": "code", "execution_count": 6, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+--------------------+\n", "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|MEDV|  assembled_features|\n", "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+--------------------+\n", "|0.00632|18.0| 2.31| 0.0|0.538|6.575| 65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98|24.0|[0.0,4.98,65.2,1....|\n", "|0.02731| 0.0| 7.07| 0.0|0.469|6.421| 78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14|21.6|[0.0,9.14,78.9,2....|\n", "|0.02729| 0.0| 7.07| 0.0|0.469|7.185| 61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03|34.7|[0.0,4.03,61.1,2....|\n", "|0.03237| 0.0| 2.18| 0.0|0.458|6.998| 45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94|33.4|[0.0,2.94,45.8,3....|\n", "|0.06905| 0.0| 2.18| 0.0|0.458|7.147| 54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33|36.2|[0.0,5.33,54.2,3....|\n", "|0.02985| 0.0| 2.18| 0.0|0.458| 6.43| 58.7|6.0622|3.0|222.0|   18.7|394.12| 5.21|28.7|[0.0,5.21,58.7,3....|\n", "|0.08829|12.5| 7.87| 0.0|0.524|6.012| 66.6|5.5605|5.0|311.0|   15.2| 395.6|12.43|22.9|[0.0,12.43,66.6,5...|\n", "|0.14455|12.5| 7.87| 0.0|0.524|6.172| 96.1|5.9505|5.0|311.0|   15.2| 396.9|19.15|27.1|[0.0,19.15,96.1,5...|\n", "|0.21124|12.5| 7.87| 0.0|0.524|5.631|100.0|6.0821|5.0|311.0|   15.2|386.63|29.93|16.5|[0.0,29.93,100.0,...|\n", "|0.17004|12.5| 7.87| 0.0|0.524|6.004| 85.9|6.5921|5.0|311.0|   15.2|386.71| 17.1|18.9|[0.0,17.1,85.9,5....|\n", "|0.22489|12.5| 7.87| 0.0|0.524|6.377| 94.3|6.3467|5.0|311.0|   15.2|392.52|20.45|15.0|[0.0,20.45,94.3,5...|\n", "|0.11747|12.5| 7.87| 0.0|0.524|6.009| 82.9|6.2267|5.0|311.0|   15.2| 396.9|13.27|18.9|[0.0,13.27,82.9,5...|\n", "|0.09378|12.5| 7.87| 0.0|0.524|5.889| 39.0|5.4509|5.0|311.0|   15.2| 390.5|15.71|21.7|[0.0,15.71,39.0,5...|\n", "|0.62976| 0.0| 8.14| 0.0|0.538|5.949| 61.8|4.7075|4.0|307.0|   21.0| 396.9| 8.26|20.4|[0.0,8.26,61.8,4....|\n", "|0.63796| 0.0| 8.14| 0.0|0.538|6.096| 84.5|4.4619|4.0|307.0|   21.0|380.02|10.26|18.2|[0.0,10.26,84.5,4...|\n", "|0.62739| 0.0| 8.14| 0.0|0.538|5.834| 56.5|4.4986|4.0|307.0|   21.0|395.62| 8.47|19.9|[0.0,8.47,56.5,4....|\n", "|1.05393| 0.0| 8.14| 0.0|0.538|5.935| 29.3|4.4986|4.0|307.0|   21.0|386.85| 6.58|23.1|[0.0,6.58,29.3,4....|\n", "| 0.7842| 0.0| 8.14| 0.0|0.538| 5.99| 81.7|4.2579|4.0|307.0|   21.0|386.75|14.67|17.5|[0.0,14.67,81.7,4...|\n", "|0.80271| 0.0| 8.14| 0.0|0.538|5.456| 36.6|3.7965|4.0|307.0|   21.0|288.99|11.69|20.2|[0.0,11.69,36.6,4...|\n", "| 0.7258| 0.0| 8.14| 0.0|0.538|5.727| 69.5|3.7965|4.0|307.0|   21.0|390.95|11.28|18.2|[0.0,11.28,69.5,4...|\n", "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+--------------------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["# Respuesta\n", "from pyspark.ml.feature import VectorAssembler\n", "\n", "variables_vector_assembler = list(set(boston.columns) - set(['MEDV'])) # We do not add the target variable\n", "vectorassembler = VectorAssembler(inputCols = variables_vector_assembler, outputCol = 'assembled_features')\n", "boston = vectorassembler.transform(boston)\n", "boston.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "#### Divisi\u00f3n train/test\n", "Realizamos la division train/test (o train/validation) para medir el desempe\u00f1o del modelo tras el ajuste."]}, {"cell_type": "code", "execution_count": 7, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "boston_train, boston_test = boston.randomSplit([0.8,0.2])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Regresi\u00f3n Lineal\n", "Modelo matem\u00e1tico usado para aproximar la relaci\u00f3n de dependencia entre una variable dependiente $Y$, las variables independientes $X_i$ y un t\u00e9rmino aleatorio $\u03b5$. Se expresa a traves de la siguiente ecuaci\u00f3n:\n", "\n", "$$Y = \\beta_0+\\beta_1*X_1+\\beta_2*X_2+...+\\beta_p*X_p+\u03b5$$\n", "\n", "donde \n", "- $Y$ es la variable dependiente, explicada o target,\n", "- $X_i$ son las variables explicativas, independientes o regresoras,\n", "- $\\beta_i$ son los parametros que miden la influencia que tienen las variables explicativas sobre el target,\n", "\n", "para todo $0<=i<=p$.\n", "\n", "Ajustamos un modelo de regresi\u00f3n y sacamos los valores reales y los predichos."]}, {"cell_type": "code", "execution_count": 8, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+----+------------------+\n", "|MEDV|        prediction|\n", "+----+------------------+\n", "|31.6| 32.67371908781077|\n", "|34.9| 33.38533918735798|\n", "|22.9| 24.97504143737227|\n", "|23.9|27.088272888403544|\n", "|22.6| 27.15961849422017|\n", "|25.0| 27.64644312692204|\n", "|21.2|21.788123942836137|\n", "|23.6|30.903239789194487|\n", "|24.1| 25.69443240323216|\n", "|22.9|23.456236064211744|\n", "|19.7| 21.39743320819953|\n", "|21.7|20.599737094317067|\n", "|28.4|28.640945117479966|\n", "|18.8|21.319644735065754|\n", "|26.6|27.885786896806316|\n", "|20.9|20.891588086501976|\n", "|19.5|18.765585748709753|\n", "|21.7| 21.84251616338134|\n", "|24.7| 24.50571357817384|\n", "|18.3|20.883807380689294|\n", "+----+------------------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["# Respuesta\n", "from pyspark.ml.regression import LinearRegression\n", "\n", "linear_regression = LinearRegression(featuresCol='assembled_features', labelCol='MEDV')\n", "linear_regression_model = linear_regression.fit(boston_train)\n", "boston_test_linear_regression = linear_regression_model.transform(boston_test)\n", "boston_test_linear_regression.select(['MEDV','prediction']).show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Arbol de Decisi\u00f3n\n", "Modelo predictivo que se construye ejecutando una partici\u00f3n binaria recursiva de los datos identificando las variables y los puntos de corte de las mismas que mejor determinan el valor de la variable objetivo. En el entrenamiento son importantes los par\u00e1metros: medida de impureza y criterio de parada (normalmente profundidad).\n", "\n", "Ajustamos un \u00e1rbol de decisi\u00f3n para nuestro conjunto de datos y sacamos los valores reales y los predichos."]}, {"cell_type": "code", "execution_count": 9, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+----+------------------+\n", "|MEDV|        prediction|\n", "+----+------------------+\n", "|31.6|  32.8095238095238|\n", "|34.9|  32.8095238095238|\n", "|22.9|         23.978125|\n", "|23.9| 27.39259259259259|\n", "|22.6|21.074789915966388|\n", "|25.0| 27.39259259259259|\n", "|21.2|21.074789915966388|\n", "|23.6|  32.8095238095238|\n", "|24.1|         23.978125|\n", "|22.9|21.074789915966388|\n", "|19.7|21.074789915966388|\n", "|21.7|21.074789915966388|\n", "|28.4| 27.39259259259259|\n", "|18.8|20.699999999999996|\n", "|26.6| 27.39259259259259|\n", "|20.9|21.074789915966388|\n", "|19.5|16.117073170731707|\n", "|21.7|21.074789915966388|\n", "|24.7|         23.978125|\n", "|18.3|20.699999999999996|\n", "+----+------------------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["# Respuesta\n", "\n", "from pyspark.ml.regression import DecisionTreeRegressor\n", "\n", "decision_tree_regression = DecisionTreeRegressor(featuresCol='assembled_features', labelCol='MEDV')\n", "decision_tree_regression_model = decision_tree_regression.fit(boston_train)\n", "boston_test_decision_tree_regression = decision_tree_regression_model.transform(boston_test)\n", "boston_test_decision_tree_regression.select(['MEDV','prediction']).show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "### Random Forest\n", "Modelo predictivo basado en \u00e1rboles de decisi\u00f3n. Construye varios \u00e1rboles tomando muestras del conjunto de datos (boosting) y muestras del conjunto de variables. Realiza la predicci\u00f3n para cada nuevo registro pasandolo por cada uno de los \u00e1rboles y promediando los resultados obtenidos.\n", "\n", "Ajustamos un random forest y obtenemos los valores reales y los predichos. "]}, {"cell_type": "code", "execution_count": 10, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+----+------------------+\n", "|MEDV|        prediction|\n", "+----+------------------+\n", "|31.6|33.042758209794464|\n", "|34.9| 37.49463427817052|\n", "|22.9|23.806929683515303|\n", "|23.9|25.067190830049377|\n", "|22.6| 24.44257786642228|\n", "|25.0| 26.73127264710039|\n", "|21.2|21.045540543591056|\n", "|23.6|31.873814731121154|\n", "|24.1|23.896402042797657|\n", "|22.9|20.822018953254847|\n", "|19.7|21.019563176280172|\n", "|21.7|20.057134466137548|\n", "|28.4|26.938687963918262|\n", "|18.8|19.945213933815605|\n", "|26.6|30.074980333582552|\n", "|20.9|21.611580564337153|\n", "|19.5| 17.93289634889239|\n", "|21.7|20.959670489576055|\n", "|24.7|24.013625085814155|\n", "|18.3|20.141509045873374|\n", "+----+------------------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["# Respuesta\n", "from pyspark.ml.regression import RandomForestRegressor\n", "\n", "random_forest = RandomForestRegressor(featuresCol='assembled_features', labelCol='MEDV')\n", "random_forest_model = random_forest.fit(boston_train)\n", "boston_test_random_forest = random_forest_model.transform(boston_test)\n", "boston_test_random_forest.select(['MEDV','prediction']).show()"]}, {"cell_type": "markdown", "metadata": {}, "source": [" \n", "\n", "# Evaluaci\u00f3n de modelos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Para comparar el desempe\u00f1o de los modelos ajustados y poder seleccionar el mejor de ellos disponemos de diversas m\u00e9tricas. Algunas de ellas son:\n", "- Error cuadr\u00e1tico medio\n", "- Ra\u00edz del error cuadr\u00e1tico medio\n", "- R cuadrado\n", "- Error absoluto medio\n", "\n", "Como ejemplo obtenemos el RMSE y el MAE de los modelos ajustados."]}, {"cell_type": "code", "execution_count": 11, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Respuesta\n", "\n", "from pyspark.ml.evaluation import RegressionEvaluator\n", "\n", "\"\"\" Possibilities::\n", "metric name in evaluation - one of:\n", "                       rmse - root mean squared error (default)\n", "                       mse - mean squared error\n", "                       r2 - r^2 metric\n", "                       mae - mean absolute error.\n", "\"\"\"\n", "\n", "rmse = RegressionEvaluator(predictionCol='prediction', labelCol='MEDV', metricName='rmse')\n", "mae = RegressionEvaluator(predictionCol='prediction', labelCol='MEDV', metricName='mae')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Imprimir m\u00e9tricas para los distintos modelos"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"collapsed": false, "scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["RMSE for a linear regression: 4.056855814284501\n", "RMSE for a decision tree: 3.410485523217875\n", "RMSE for a random forest: 2.798164095399894\n", "\n", "MAE for a regresi\u00f3n lineal: 3.054713938595181\n", "MAE for a decision tree: 2.409132195678887\n", "MAE for a random forest: 2.0325956959757283\n"]}], "source": ["# Respuesta\n", "\n", "print(\"RMSE for a linear regression: {}\".format(rmse.evaluate(boston_test_linear_regression)))\n", "print(\"RMSE for a decision tree: {}\".format(rmse.evaluate(boston_test_decision_tree_regression)))\n", "print(\"RMSE for a random forest: {}\\n\".format(rmse.evaluate(boston_test_random_forest)))\n", "\n", "print(\"MAE for a regresi\u00f3n lineal: {}\".format(mae.evaluate(boston_test_linear_regression)))\n", "print(\"MAE for a decision tree: {}\".format(mae.evaluate(boston_test_decision_tree_regression)))\n", "print(\"MAE for a random forest: {}\".format(mae.evaluate(boston_test_random_forest)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "Observando los valores obtenidos en las m\u00e9tricas de evaluaci\u00f3n de modelos decidir\u00edamos quedarnos con el random forest."]}], "metadata": {"@webio": {"lastCommId": "359b4713c0ec4b149ad16c0fc2069df1", "lastKernelId": "49b0212b-4f85-49da-b42c-a02242f6cb07"}, "kernelspec": {"display_name": "PySpark 2.4 (python 3.7)", "language": "python", "name": "pyspark2.4"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.2"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}