{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SANDBOX_NAME = # Sandbox Name\n",
    "DATA_PATH = \"/data/sandboxes/\"+SANDBOX_NAME+\"/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Spark ML Pipelines\n",
    "\n",
    "Cargamos un dataset con información sobre cuán seguro es un coche. Con este dataset se estudiarán funciones muy importantes de Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Crear SparkSession\n",
    "\n",
    "Sabemos que en Datio no es necesario crear la sesión de Spark ya al iniciar un notebook con el Kernel PySpark Python3 - Spark 2.1.0 se crea automáticamente. Pero así lo haríamos si fuera necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Cargar datos y comprobar schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cars = spark.read.csv(DATA_PATH+'automobile.csv', sep=';', header=True, inferSchema=True)\n",
    "\n",
    "cars.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cars.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Vamos a trabajar con los valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Una manera de devolver un dataframe sin filas que contengan nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "cars.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Si quisieramos reemplazar los valores nulos con otro valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "cars.na.fill(50).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Otra forma de eliminar las filas con valores nulos (filtrar los nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "for column in cars.columns: \n",
    "    num_nulls = cars.where(F.col(column).isNull()).count()\n",
    "    \n",
    "    if num_nulls != 0:\n",
    "        cars = cars.where(F.col(column).isNotNull())\n",
    "        print(\"The column '{}' has {} nulls\".format(column,num_nulls))\n",
    "        print(\"The column '{}' has no more null values\".format(column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Cambiamos la variable objetivo para hacerla binaria, para poder utilizar algoritmos de clasificación binaria. Tomaremos que -2,-1,0 significa que no es muy seguro. 1,2,3 que sí es seguro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "cars = cars.withColumn('symboling_binary', F.udf(lambda value: 0. if value <= 0 else 1., DoubleType())(F.col('symboling')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Supongamos que queremos pasar la columna 'make' a dummy y luego lanzar un modelo de clasificación.**\n",
    "\n",
    "**Resolviendo sin Pipeline** \n",
    "\n",
    "Tal como hemos visto antes, esto lo podríamos hacer paso a paso. \n",
    "\n",
    "1. Hacemos el String Indexer a la columna make para pasarla a numérica (0... n_categorias-1)\n",
    "2. Hacemos el One Hot Encoder sobre el resultado del paso anterior para hacerla dummy\n",
    "3. Seleccionamos las variables que vamos a incluir en nuestro modelo (todas aquellas que no sean string y que no sean la variable objetivo -symboling-). Eso lo hacemos recorriendo df.dtypes, el cual nos devuelve una lista de tuplas, dónde cada tupla tiene (nombre_variable, tipo_variable).\n",
    "4. Con las variables seleccionadas como predictoras del modelo, hacemos el vector assembler\n",
    "5. Dividimos train y test\n",
    "6. La salida del vector assembler será lo que le demos al modelo (en este caso un Random Forest). Entrenamos (*fit*) y predecimos (*transform*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='make', outputCol='make_indexed')\n",
    "string_indexer_model = string_indexer.fit(cars)\n",
    "cars_many_steps = string_indexer_model.transform(cars)\n",
    "\n",
    "onehotencoder = OneHotEncoder(dropLast=False, inputCol= string_indexer.getOutputCol(), outputCol='make_encoded')\n",
    "cars_many_steps = onehotencoder.transform(cars_many_steps)\n",
    "\n",
    "columns_for_model = [element[0] for element in cars_many_steps.dtypes if element[1] != 'string' and 'symboling' not in element[0]]\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=columns_for_model, outputCol='assembled_features')\n",
    "cars_many_steps = vector_assembler.transform(cars_many_steps)\n",
    "\n",
    "\n",
    "cars_many_steps_train, cars_many_steps_test = cars_many_steps.randomSplit([0.8,0.2])\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=vector_assembler.getOutputCol(), labelCol='symboling_binary')\n",
    "rf_model = rf.fit(cars_many_steps_train)\n",
    "cars_many_steps = rf_model.transform(cars_many_steps_test)\n",
    "\n",
    "cars_many_steps.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Resolviendo con Pipeline** \n",
    "\n",
    "Es parecido a lo que hicimos sin pipeline, pero en lugar de hacer: \n",
    "- Creo el objeto string indexer, hago *fit*, hago *transform*\n",
    "- Creo el objeto OHE, hago *transform*\n",
    "- Etc. \n",
    "\n",
    "Lo que hago es simplemente crear los objetos, los meto como *stages* del pipeline, y luego le hago *fit* y *transform* al pipeline. Vamos a verlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='make', outputCol='make_indexed')\n",
    "\n",
    "onehotencoder = OneHotEncoder(dropLast=False, inputCol= string_indexer.getOutputCol(), outputCol='make_encoded')\n",
    "\n",
    "columns_for_model = [element[0] for element in cars.dtypes if element[1] != 'string' and 'symboling' not in element[0]] + [onehotencoder.getOutputCol()]\n",
    "vector_assembler = VectorAssembler(inputCols=columns_for_model, outputCol='assembled_features')\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=vector_assembler.getOutputCol(), labelCol='symboling_binary')\n",
    "\n",
    "# Until here is the same as without pipeline but without doing fit / transform\n",
    "# We use this objects as stages for our pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, onehotencoder, vector_assembler, rf])\n",
    "\n",
    "cars_pipeline_train, cars_pipeline_test = cars.randomSplit([0.8,0.2])\n",
    "\n",
    "# We do fit and transform on the pipeline object\n",
    "\n",
    "pipeline_model = pipeline.fit(cars_pipeline_train)\n",
    "\n",
    "cars_pipeline = pipeline_model.transform(cars_pipeline_test)\n",
    "\n",
    "cars_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Los pipelines nos permiten reproducir todo el flujo cada vez que tenemos nuevos datos, de esta manera nos aseguramos de que cada nuevo \"batch\" se somete exactamente al mismo procesado.\n",
    "\n",
    "**Para guardar el pipeline completo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "pipeline_name = \"random_forest_pipeline\"\n",
    "models_path = DATA_PATH + \"models/\"\n",
    "pipeline_model.save(models_path + pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "**Podemos despues cargar el pipeline de la siguiente manera**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "loaded_pipelinemodel = PipelineModel.load(models_path + pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Ejercicio 1\n",
    "\n",
    "Dado el siguiente DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col as c\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer, StringIndexerModel\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "df = spark.read.csv(DATA_PATH + 'pokemon.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1) Defina una función que construya un `pipeline` con los pasos necesarios para preparar los datos y que aplique dicho pipeline a un DataFrame que reciba como parámetro de la función.\n",
    "\n",
    "2) Coonstruya un modelo de clasificación usando `pyspark.ml.classification.RandomForestClassifier` para predecir si la velocidad de un pokemon es mayor a la media.\n",
    "\n",
    "3) Extraiga la probabilidad de que la predicción sea 1 en una columna separada.\n",
    "\n",
    "4) Construya una función que calcule los valores de *precision* y *recall* para diferentes valores de umbral (*threshold*). Es decir, calcule  el valor de las métricas para valores de umbral entre 0 y 1.\n",
    "\n",
    "**Extra**: Dibuje las curvas de *precision* y *recall* versus el umbral (eje x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "def preprocess_to_model(df, targetCol, path, subset=None, scaling=None, mode=True):\n",
    "    assert scaling in (None, 'MinMax', 'Standard')\n",
    "    \n",
    "    if subset is None:\n",
    "        subset = [col for col in df.columns if col != targetCol]\n",
    "    \n",
    "    df = df.select(subset + [targetCol])\n",
    "    \n",
    "    categorical_cols = [col for col, type_ in df.dtypes \n",
    "                        if type_ == 'string'\n",
    "                        and col != targetCol]\n",
    "    boolean_cols = [col for col, type_ in df.dtypes \n",
    "                    if type_ == 'boolean'\n",
    "                    and col != targetCol]\n",
    "    numerical_cols = [col for col in df.columns \n",
    "                      if col not in categorical_cols\n",
    "                      and col not in boolean_cols\n",
    "                      and col != targetCol]\n",
    "    \n",
    "    str_idxs = [StringIndexer(inputCol=col, outputCol=col+'_idx', handleInvalid='skip')\n",
    "                for col in categorical_cols] # handleInvalid='keep'\n",
    "    \n",
    "    ohes = [OneHotEncoder(inputCol=col+'_idx', outputCol=col+'_ohe')\n",
    "            for col in categorical_cols]\n",
    "    \n",
    "    if scaling is not None:\n",
    "        pre_scaling_assembler = VectorAssembler(inputCols=numerical_cols, \n",
    "                                                outputCol='numerical_features')\n",
    "        \n",
    "        if scaling == 'Standard':\n",
    "            scaler = StandardScaler(withMean=True, withStd=True, \n",
    "                                    inputCol='numerical_features', \n",
    "                                    outputCol='numerical_features_scaled')\n",
    "        else:\n",
    "            scaler = MinMaxScaler(inputCol='numerical_features', \n",
    "                                    outputCol='numerical_features_scaled')\n",
    "            \n",
    "        assembler = VectorAssembler(inputCols=[col+'_ohe' for col in categorical_cols] + \\\n",
    "                                              ['numerical_features_scaled'] + \\\n",
    "                                              boolean_cols, \n",
    "                                outputCol='features')\n",
    "        \n",
    "        stages = str_idxs+ohes+[pre_scaling_assembler,scaler, assembler]\n",
    "        \n",
    "    else:\n",
    "        assembler = VectorAssembler(inputCols=[col+'_ohe' for col in categorical_cols] + \\\n",
    "                                              numerical_cols + boolean_cols, \n",
    "                                outputCol='features')\n",
    "        \n",
    "        stages = str_idxs+ohes+[assembler]\n",
    "    \n",
    "    if mode:\n",
    "        pipeline = Pipeline(stages=stages).fit(df)\n",
    "        pipeline.save(path)\n",
    "        print('Saving pipeline object on ' + path)\n",
    "    else:\n",
    "        pipeline = PipelineModel.load(path)\n",
    "        print('Loading pipeline object from ' + path)\n",
    "    \n",
    "    df = pipeline.transform(df)\n",
    "    \n",
    "    return df.select('features', targetCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "for col in df.columns:\n",
    "    df = df.withColumnRenamed(col, col.lower().replace('.', '').replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "target = 'target'\n",
    "\n",
    "avg = df.agg(F.avg('speed').alias('speed')).first()['speed']\n",
    "\n",
    "df = df.withColumn('target', (c('speed') > avg).cast('double'))  # Both lines do the same\n",
    "df = df.withColumn('target', F.when(c('speed') > avg, F.lit(1.0)).otherwise(F.lit(0.0)))    # Both lines do the same\n",
    "\n",
    "df = df.drop('speed', 'sp_atk', 'sp_def', 'type_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "print(df.count())\n",
    "df = df.dropna(how='any')\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "df_train, df_test = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "subset = [col for col in df_train.columns if col != 'name']\n",
    "\n",
    "df_train_preprocesed = preprocess_to_model(df_train, target, '4', subset=subset,\n",
    "                                           scaling=None, mode=True)\n",
    "\n",
    "df_test_preprocesed = preprocess_to_model(df_test, target, '4', subset=subset,\n",
    "                                          scaling=None, mode=False)\n",
    "\n",
    "df_train_preprocesed.cache()\n",
    "df_test_preprocesed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "forest_classifier = RandomForestClassifier(featuresCol='features', labelCol=target)\n",
    "forest_classifier = forest_classifier.fit(df_train_preprocesed)\n",
    "df_forest_predicted = forest_classifier.transform(df_test_preprocesed)\n",
    "\n",
    "df_forest_predicted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "vector_to_array_udf = F.udf(lambda x: x.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "df_forest_predicted = df_forest_predicted.withColumn('probability',\n",
    "                                                         vector_to_array_udf(c('probability')))\n",
    "df_forest_predicted = df_forest_predicted.withColumn('probability',  c('probability')[1])\n",
    "df_forest_predicted.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics_by_threshold(df, pred_col, target, n_thresholds=99):\n",
    "    df = df.select(pred_col, target)\n",
    "    \n",
    "    # explode by threshold\n",
    "    thresholds = [np.round(x, 2) for x in np.linspace(0.01, 0.99, n_thresholds)]\n",
    "    \n",
    "    df = df.withColumn('threshold', F.array(*[F.lit(threshold) for threshold in thresholds]))\n",
    "    df = df.withColumn('threshold', F.explode(c('threshold')))\n",
    "        \n",
    "    # calculating metrics\n",
    "    df = df.withColumn('PP', c(pred_col) > c('threshold'))\n",
    "    df = df.withColumn('TP', c('PP') & c(target).cast('boolean'))\n",
    "    agg_df = df.groupBy('threshold').agg(F.sum(c('PP').cast('int')).alias('PP'),\n",
    "                                         F.sum(c('TP').cast('int')).alias('TP'),\n",
    "                                         F.sum(c(target).cast('int')).alias('RP'))\n",
    "    agg_df = agg_df.withColumn('precision', c('TP')/c('PP'))\n",
    "    agg_df = agg_df.withColumn('recall', c('TP')/c('RP'))\n",
    "    agg_df = agg_df.withColumn('f1_score', 2 * (c('precision') * c('recall')) / (c('precision') + c('recall')))\n",
    "\n",
    "    agg_df = agg_df.select('threshold', 'precision', 'recall', 'f1_score')\n",
    "        \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "df.select(c(target).cast('boolean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "by_threshold = calculate_metrics_by_threshold(df_forest_predicted, \n",
    "                                              'probability', target, n_thresholds=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "by_threshold = by_threshold.orderBy('threshold').toPandas()\n",
    "by_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Respuesta\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "by_threshold.plot(x='threshold', y=['precision', 'recall', 'f1_score'], figsize=(15,7))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "5b3af32b012d4cf18500bf3c0eb2793e",
   "lastKernelId": "280ee430-1e8a-4ee5-b24f-9dda02bcdab5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
